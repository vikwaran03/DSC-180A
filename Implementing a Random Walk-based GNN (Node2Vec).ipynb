{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f016489f",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: \n",
    "\n",
    "In this assignment, implement the Node2Vec algorithm, a random-walk-based GNN, to learn node embeddings. Train a classifier using the learned embeddings to predict node labels.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Cora dataset: The dataset consists of 2,708 nodes (scientific publications) with 5,429 edges (citations between publications). Each node has a feature vector of size 1,433, and there are 7 classes (research topics).\n",
    "Skeleton Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c492a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b30b6c022a40f6b15a31b227ad3f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:31<00:00,  3.21it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:33<00:00,  2.98it/s]\n",
      "/tmp/ipykernel_8318/523172613.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0427963733673096\n",
      "Epoch 10, Loss: 1.2911992073059082\n",
      "Epoch 20, Loss: 0.9227162599563599\n",
      "Epoch 30, Loss: 0.7650074362754822\n",
      "Epoch 40, Loss: 0.6909422278404236\n",
      "Epoch 50, Loss: 0.647700846195221\n",
      "Epoch 60, Loss: 0.6177152991294861\n",
      "Epoch 70, Loss: 0.5954135656356812\n",
      "Epoch 80, Loss: 0.578117847442627\n",
      "Epoch 90, Loss: 0.5642012357711792\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec  # Importing Node2Vec for the random walk\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "         print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ee022",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Explanation:\n",
    "Node2Vec generates node embeddings by simulating random walks on the graph. These walks capture structural properties of nodes.\n",
    "The generated embeddings are then used to train a classifier for predicting node labels.\n",
    "The Cora dataset is a benchmark graph where nodes are papers and edges are citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b3004",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "4. What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "5. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "6. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "8. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "\n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "\n",
    "## No points, just for you to think about\n",
    "7. What would happen if we removed self-loops from the graph before training Node2Vec?\n",
    "\n",
    "9. What would happen if we applied normalization to the node embeddings before feeding them to the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113533e-51da-41cc-971f-9692cb3407cd",
   "metadata": {},
   "source": [
    "### 1) What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "\n",
    "If we increase the number of walks per node, I would hypothesize the embeddings to become strong since know there is more node neighborhoods being constructed, resulting in a stronger understanding of graph structure. This should then be reflected within the embeddings and the classifier should have a lower loss than before. \n",
    "\n",
    "To test this, I am simply going to increase the number of walks from 200 to 400 (double it) and see how the classifier performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c60fa69-4701-4f60-8561-0e262c28fca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292887d2fb5f448e9863e2d4a13aaf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 200/200 [01:05<00:00,  3.06it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 200/200 [01:14<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9518219232559204\n",
      "Epoch 10, Loss: 1.2408803701400757\n",
      "Epoch 20, Loss: 0.8988996148109436\n",
      "Epoch 30, Loss: 0.7474140524864197\n",
      "Epoch 40, Loss: 0.6745575070381165\n",
      "Epoch 50, Loss: 0.632602870464325\n",
      "Epoch 60, Loss: 0.6045637726783752\n",
      "Epoch 70, Loss: 0.583960235118866\n",
      "Epoch 80, Loss: 0.5679291486740112\n",
      "Epoch 90, Loss: 0.5549281239509583\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=400, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf75677-e554-40ed-bf96-da660b4b03be",
   "metadata": {},
   "source": [
    "Based off the results, you can tell that my hypothesis was indeed correct and the loss did decrease from .564 to .554. This is due to the reasons I stated above relating how with an increased number of walks there is also an increased understanding of the graph structure and in turn better node2vec embeddings leading to better classification accuracy by the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab38d48-ac6d-40e8-834b-ae610fab93c9",
   "metadata": {},
   "source": [
    "### 2) What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "If we reduced the walk length, this would influence the structural information captured by the embeddings due to the fact the there is a smaller magnitude of information being captured. For example, 200 walks of length 30 will yield 200 different node neighborhoods of 30 nodes, but 200 walks of length 10 will only yield 200 different node neighborhoods of 10. There is an upperbound for how high the walk length and num walks can go, but other than that decreasing the length of the walk will only capture less information resulting in weaker spectral embeddings. \n",
    "\n",
    "To test this, I will simply run the same model as the initial modeel with 200 num_walks, but change the walk_length to 10 instead of 30. We should see an increase in loss by making this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a09f8e-b83e-45ac-bcab-659f1a4c44ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3d7471820f41eab2792249824b6916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:09<00:00, 10.06it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:10<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.054508924484253\n",
      "Epoch 10, Loss: 1.3165380954742432\n",
      "Epoch 20, Loss: 0.952644944190979\n",
      "Epoch 30, Loss: 0.7927368879318237\n",
      "Epoch 40, Loss: 0.7130237221717834\n",
      "Epoch 50, Loss: 0.6647894978523254\n",
      "Epoch 60, Loss: 0.6317962408065796\n",
      "Epoch 70, Loss: 0.6075016856193542\n",
      "Epoch 80, Loss: 0.5886807441711426\n",
      "Epoch 90, Loss: 0.5735780000686646\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=10, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "         print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca165e-be7f-45ce-9ea9-0ce37efce58a",
   "metadata": {},
   "source": [
    "As expected, my hypothesis was correct and we did in fact see an increase in loss from the initial model 0.564 to 0.574 (walk_length = 10). Like mentioned, this is due to less information being captured in the embeddings, leading to worse classifications by the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799eda7-f836-41c2-9ee7-ffe8a3847b51",
   "metadata": {},
   "source": [
    "### 3) What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "\n",
    "To test this idea, I am changing the to_undirected variables in to_networkx to False, to make the graph directed. I want to see how the classifier performance is going to be affected by this change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a224a64-41b9-4106-8ecb-4bd0ee0cd5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ed9831b9994722b7dd41b14f247d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:33<00:00,  3.01it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:35<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0894391536712646\n",
      "Epoch 10, Loss: 1.3139939308166504\n",
      "Epoch 20, Loss: 0.9534384608268738\n",
      "Epoch 30, Loss: 0.7869365215301514\n",
      "Epoch 40, Loss: 0.7068240642547607\n",
      "Epoch 50, Loss: 0.6608007550239563\n",
      "Epoch 60, Loss: 0.6296052932739258\n",
      "Epoch 70, Loss: 0.606698215007782\n",
      "Epoch 80, Loss: 0.5887612104415894\n",
      "Epoch 90, Loss: 0.5742022395133972\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec  # Importing Node2Vec for the random walk\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "\n",
    "G = to_networkx(data, to_undirected=False)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "         print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51893321-7e16-4c4d-9646-6da1ad19205e",
   "metadata": {},
   "source": [
    "This model has the exact same parameters as the beggining model with walk length of 30 and num walks set to 2000, the only thing that changed was changing the graphs from undirected to directed. By making this change, you can see there has been a .01 increase in loss from .564 (undirected) to .574 (directed). This is in a large part due to the fact in how embeddings in node2vec are being created now that the graph has only directed edges. With undirected edges, the walk length of 30 was able to act freely as it new all nodes with edges are neighbors, but now since that constraint is not there, the graph becomes harder to read and more volatile with changes. Due to this, the embeddings made in node2vec are not as strong as the ones made when the graph was undirected. Since the classifier is all dependent on the quality of the embeddings, this clearly explains why there was an increase in loss when changing the graph from undirected to directed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630139a-c4e4-4a3a-bcc1-5061de674f68",
   "metadata": {},
   "source": [
    "### 4) What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "\n",
    "If we added more features to the nodes, i.e. increasing each node's feature vector from 1433 to 2000, there wouldn't be any change on the node2vec embedding, hence no change on the overall classifier. This is in a larger part due to how the node2vec embeddings are being created. In the code we started with, each node is going to have 200 random walks of length 30 performed and while doing so information regarding the structure of the graph will be revealed. Nothing regarding the actual feature vectors of the graph is going to be affected. Hence, we don't even need the feature vector, in a way, as we are just looking for graph structure and node neighborhood and not any actual tangible information regarding the nodes. That would be used more in a message-passing algorithm. \n",
    "\n",
    "No code for this description at I believe it to pretty intuitive. If I were to change each feature vector, I would pad them all with 0s and I would expect any changes in the loss be due to probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bee51a-0938-40d2-9144-17ae97208e7a",
   "metadata": {},
   "source": [
    "### 5) What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc479e64-a4c0-4846-978a-3ac10c149b1c",
   "metadata": {},
   "source": [
    "If we were to run the same code on a different dataset with more classes the classifier performance would go down in accuracy. To start with creating the embeddings, nothing in terrms of the algorithm will change, however the resulting 64-dimensional embeddings will become less unique and as a result make the classifer perform worse. The embeddings will be less distinct due to there being more classes in the graph that may not all be covered the same amount during a random walk since it is due to probability. Since the classifiers are being trained off of the embeddings from node2vec, the more unique the embedddings are the better the classifier will be at predicting the class. Now, there are more classes, hence less robust embeddings causing a decrease in classifier performance.\n",
    "\n",
    "No code was also shown for this due to not specifying in the question itself. However, it would not be very difficult to show. I would simply run the exact same model, with some minor tweaks like changing the output dimension. But, other than that it should be concrete enough to see that the embeddings will be worsened, although staying the same length, and the classifier performance will decrease (probably not significantly, but depends on how many new classes are added). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f0319-784d-49e4-aea1-f6eedc5ea599",
   "metadata": {},
   "source": [
    "### 6) What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "\n",
    "If we were to enlarge the embedding dimensions from 64 to 128, there should be an increase in the model's performance. If there is any decrease, it is probably due to overfitting on the training data or the embedding picking up unnecessary information due to having more information than needed (depending on the graph itself). I would assume the training time to increase significantly. \n",
    "\n",
    "To test this, I will change the embedding dimension to 128, time how long it runs, and also check if the accuracy on the test increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff8ed8fa-d78d-490a-b8d0-83212b397476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d739881e0cbd41b29eb2f4d2b965fa10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:32<00:00,  3.03it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:36<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0100767612457275\n",
      "Epoch 10, Loss: 1.283320665359497\n",
      "Epoch 20, Loss: 0.9119169116020203\n",
      "Epoch 30, Loss: 0.755591630935669\n",
      "Epoch 40, Loss: 0.6841944456100464\n",
      "Epoch 50, Loss: 0.6437552571296692\n",
      "Epoch 60, Loss: 0.6167096495628357\n",
      "Epoch 70, Loss: 0.5968500971794128\n",
      "Epoch 80, Loss: 0.581413745880127\n",
      "Epoch 90, Loss: 0.5688578486442566\n",
      "Training complete!\n",
      "total time: 470.3285357952118\n"
     ]
    }
   ],
   "source": [
    "# 64 dimensional embedding\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "         print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "end = time.time()\n",
    "print(f\"total time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "320acb02-c4c2-4960-9d87-0307a2e00625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab4468bfb92491cadff7d4c63995aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:33<00:00,  2.98it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:37<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9145219326019287\n",
      "Epoch 10, Loss: 1.0649425983428955\n",
      "Epoch 20, Loss: 0.7508973479270935\n",
      "Epoch 30, Loss: 0.6398661136627197\n",
      "Epoch 40, Loss: 0.5815190076828003\n",
      "Epoch 50, Loss: 0.5418511629104614\n",
      "Epoch 60, Loss: 0.5128682255744934\n",
      "Epoch 70, Loss: 0.4906698763370514\n",
      "Epoch 80, Loss: 0.4728759229183197\n",
      "Epoch 90, Loss: 0.4580802917480469\n",
      "Training complete!\n",
      "total time: 486.28948187828064\n"
     ]
    }
   ],
   "source": [
    "# 128 dimensional embedding\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=128, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(128, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "         print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "end = time.time()\n",
    "print(f\"total time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e526edba-c14b-4018-8236-9b1fd814bb48",
   "metadata": {},
   "source": [
    "The resuls are for the embeddings with 64 dimension vectors took 470 seconds to train and had a loss of .569 while the embeddings with 128 dimension vectors took 486 seconds and the loss was .458. Hence, my prediction that the accuracy of the classifier was correct, it was in fact a lot better. My training time prediction was still correct, but not nearly as much as I thought. Only 16 more seconds is nowhere near what I expected, so it definetely makes sense to use the higher dimensional embedding vectors as long as it is not overfit on training data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
