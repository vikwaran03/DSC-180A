{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d48fe682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9507317543029785\n",
      "Epoch 10, Loss: 0.666307270526886\n",
      "Epoch 20, Loss: 0.127598837018013\n",
      "Epoch 30, Loss: 0.02733098715543747\n",
      "Epoch 40, Loss: 0.009782478213310242\n",
      "Epoch 50, Loss: 0.005362648516893387\n",
      "Epoch 60, Loss: 0.0037494285497814417\n",
      "Epoch 70, Loss: 0.0029849072452634573\n",
      "Epoch 80, Loss: 0.0025219605304300785\n",
      "Epoch 90, Loss: 0.002197231398895383\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18442",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "GCN aggregates features from a node’s neighbors using graph convolutions. This allows the network to learn representations based on both node features and graph structure.\n",
    "The Cora dataset is used to classify nodes into one of 7 research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb882b",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "\n",
    "1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "\n",
    "4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "\n",
    "Extra credit: \n",
    "1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?\n",
    "\n",
    "## No points, just for you to think about:\n",
    "1. What would happen if we applied dropout to the node features during training? How would it affect the model’s generalization?\n",
    "2. What would happen if we used mean-pooling instead of summing the messages in the GCN layers?\n",
    "3. What would happen if we pre-trained the node features using a different algorithm, like Node2Vec, before feeding them into the GCN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1440f0-f482-4e58-b104-8bb8d1dac72b",
   "metadata": {},
   "source": [
    "### 1) What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "\n",
    "It is too my understanding that there is an upper-bound of how many GCN layers you can add before the network is affected by over-smoothing. My initial hypothesis is that you will eventually reach a certain depth in GCN layers where the accuracy will start to decrease on the test-set, even though the model will become stronger on the train set (which makes sense). \n",
    "\n",
    "To test this, I plan on training a GCN model with >2 convulutional layers until I find that the model starts to perform worse on the test-set. I will also need to write code to test the model on the testing set and find the accuracy to do the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bbfb6cf-3b97-4b4a-8899-48ff57cf16c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 2 GCN Layers: Epoch 0, Loss: 2.9703166484832764, Accuracy: 0.17299999296665192\n",
      "Model with 5 GCN Layers: Epoch 0, Loss: 2.9703166484832764, Accuracy: 0.4059999883174896\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 10, Loss: 1.0429617166519165, Accuracy: 0.6549999713897705\n",
      "Model with 5 GCN Layers: Epoch 10, Loss: 1.0429617166519165, Accuracy: 0.6629999876022339\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 20, Loss: 0.2774021029472351, Accuracy: 0.7770000100135803\n",
      "Model with 5 GCN Layers: Epoch 20, Loss: 0.2774021029472351, Accuracy: 0.7490000128746033\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 30, Loss: 0.07073476165533066, Accuracy: 0.7860000133514404\n",
      "Model with 5 GCN Layers: Epoch 30, Loss: 0.07073476165533066, Accuracy: 0.7239999771118164\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 40, Loss: 0.02347044087946415, Accuracy: 0.7799999713897705\n",
      "Model with 5 GCN Layers: Epoch 40, Loss: 0.02347044087946415, Accuracy: 0.718999981880188\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 50, Loss: 0.011639274656772614, Accuracy: 0.7839999794960022\n",
      "Model with 5 GCN Layers: Epoch 50, Loss: 0.011639274656772614, Accuracy: 0.7250000238418579\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 60, Loss: 0.007498366292566061, Accuracy: 0.7850000262260437\n",
      "Model with 5 GCN Layers: Epoch 60, Loss: 0.007498366292566061, Accuracy: 0.7250000238418579\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 70, Loss: 0.005621069576591253, Accuracy: 0.7820000052452087\n",
      "Model with 5 GCN Layers: Epoch 70, Loss: 0.005621069576591253, Accuracy: 0.7250000238418579\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 80, Loss: 0.004564371425658464, Accuracy: 0.7799999713897705\n",
      "Model with 5 GCN Layers: Epoch 80, Loss: 0.004564371425658464, Accuracy: 0.7250000238418579\n",
      "\n",
      "\n",
      "Model with 2 GCN Layers: Epoch 90, Loss: 0.0038649856578558683, Accuracy: 0.781000018119812\n",
      "Model with 5 GCN Layers: Epoch 90, Loss: 0.0038649856578558683, Accuracy: 0.7279999852180481\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Writing a function to test model accuracy on unseen testing data\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(data)[data.test_mask].argmax(1)\n",
    "        actuals = data.y[data.test_mask]\n",
    "        return float(sum(preds == actuals) / len(preds))\n",
    "\n",
    "k = 5 \n",
    "\n",
    "# Define a k-layer GCN \n",
    "class GCN_k(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        hidden_dim2 = int((2/3 * hidden_dim) + 10) # Found this hidden dimension general formula online\n",
    "        hidden_dim3 = int((2/3 * hidden_dim2) + 10)\n",
    "        hidden_dim4 = int((2/3 * hidden_dim3) + 10)\n",
    "        super(GCN_k, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, hidden_dim3)\n",
    "        self.conv4 = GCNConv(hidden_dim3, hidden_dim4)\n",
    "        self.conv5 = GCNConv(hidden_dim4, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model with 2 GCN layers (same as the intitial given model)\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize model with k GNC layers (everything is the same as the initial model but with more GCN layers)\n",
    "model_k = GCN_k(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer_k = optim.Adam(model_k.parameters(), lr=0.01)\n",
    "criterion_k = nn.CrossEntropyLoss()\n",
    "        \n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model_k.train()\n",
    "    optimizer_k.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    out_k = model_k(data)\n",
    "    loss_k = criterion_k(out_k[data.train_mask], data.y[data.train_mask])\n",
    "    loss_k.backward()\n",
    "    optimizer_k.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Model with 2 GCN Layers: Epoch {epoch}, Loss: {loss.item()}, Accuracy: {test(model, data)}')\n",
    "        print(f'Model with {k} GCN Layers: Epoch {epoch}, Loss: {loss.item()}, Accuracy: {test(model_k, data)}')\n",
    "        print('\\n')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d3e89-4a60-42b5-a31c-4a5f078e2250",
   "metadata": {},
   "source": [
    "As expected, you can see that by Epoch 90, the GCN with 5 layers has performed worse than the GCN with just 2 layers on the testing data. This is due to the oversmoothing of the network due to depth of training the model to conform to the training data. Although, this is probably not enough testing to say so for certain, the implication is there that it is in fact possible. \n",
    "\n",
    "After some additional research, I found a paper where it was proved that is indeed not possible to overcome the oversmoothing of GNNs with additional layers due to the fact that if you keep adding layers then eventually every node will be represented by the same feature vector which will lead to a very generalized and overfit model on the training data. \n",
    "\n",
    "This is the paper: https://papers.nips.cc/paper_files/paper/2023/file/6e4cdfdd909ea4e34bfc85a12774cba0-Paper-Conference.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bce7f8-5c61-4903-bd84-be76d0c4875d",
   "metadata": {},
   "source": [
    "### 2) What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "\n",
    "My initial thinking leads me to hypothesize that if you have a larger hidden dimension, then the model wil be likeley be overtrained on the training data and will in turn not be a good model to be used on unseen testing data. I think this because with a larger hidden dimension, the network is able to do more computations and recognize more patterns within the training data. If this dimension is too large, then the network will do a poor job of identifying the same patterns in other data. \n",
    "\n",
    "To test this, I'm going to run 2 models both with 2 GCN layers, where one model will have a hidden layer of dimension 64 and the other model will have a hidden layer of dimension 16. With this, I will be able to see whether my hypothesis is right based off of the model accuracy on testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c47182a0-f9ae-48a4-a056-bb2aaff0e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with hidden dimension 16: Epoch 0, Loss: 2.9961729049682617, Accuracy: 0.29100000858306885\n",
      "Model with hidden dimension 64: Epoch 0, Loss: 2.9961729049682617, Accuracy: 0.4830000102519989\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 10, Loss: 1.4488096237182617, Accuracy: 0.6840000152587891\n",
      "Model with hidden dimension 64: Epoch 10, Loss: 1.4488096237182617, Accuracy: 0.8050000071525574\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 20, Loss: 0.7292775511741638, Accuracy: 0.7059999704360962\n",
      "Model with hidden dimension 64: Epoch 20, Loss: 0.7292775511741638, Accuracy: 0.7940000295639038\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 30, Loss: 0.5014190673828125, Accuracy: 0.7229999899864197\n",
      "Model with hidden dimension 64: Epoch 30, Loss: 0.5014190673828125, Accuracy: 0.7799999713897705\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 40, Loss: 0.45806989073753357, Accuracy: 0.7319999933242798\n",
      "Model with hidden dimension 64: Epoch 40, Loss: 0.45806989073753357, Accuracy: 0.7839999794960022\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 50, Loss: 0.44069817662239075, Accuracy: 0.7300000190734863\n",
      "Model with hidden dimension 64: Epoch 50, Loss: 0.44069817662239075, Accuracy: 0.7839999794960022\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 60, Loss: 0.4351782500743866, Accuracy: 0.7310000061988831\n",
      "Model with hidden dimension 64: Epoch 60, Loss: 0.4351782500743866, Accuracy: 0.7850000262260437\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 70, Loss: 0.43271777033805847, Accuracy: 0.7260000109672546\n",
      "Model with hidden dimension 64: Epoch 70, Loss: 0.43271777033805847, Accuracy: 0.7850000262260437\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 80, Loss: 0.4315394461154938, Accuracy: 0.7279999852180481\n",
      "Model with hidden dimension 64: Epoch 80, Loss: 0.4315394461154938, Accuracy: 0.7850000262260437\n",
      "\n",
      "\n",
      "Model with hidden dimension 16: Epoch 90, Loss: 0.430780827999115, Accuracy: 0.7279999852180481\n",
      "Model with hidden dimension 64: Epoch 90, Loss: 0.430780827999115, Accuracy: 0.7839999794960022\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with 2 GCN layers (same as the intitial given model)\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize model with 2 GCN layers but with a 64 dimension hidden layer\n",
    "model_hidden64 = GCN_k(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "optimizer_hidden64 = optim.Adam(model_hidden64.parameters(), lr=0.01)\n",
    "criterion_hidden64 = nn.CrossEntropyLoss()\n",
    "        \n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model_hidden64.train()\n",
    "    optimizer_hidden64.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    out_hidden64 = model_hidden64(data)\n",
    "    loss_hidden64 = criterion_hidden64(out_hidden64[data.train_mask], data.y[data.train_mask])\n",
    "    loss_hidden64.backward()\n",
    "    optimizer_hidden64.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Model with hidden dimension 16: Epoch {epoch}, Loss: {loss.item()}, Accuracy: {test(model, data)}')\n",
    "        print(f'Model with hidden dimension 64: Epoch {epoch}, Loss: {loss.item()}, Accuracy: {test(model_hidden64, data)}')\n",
    "        print('\\n')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd972bc-835b-4761-940d-7b22f00a0aa8",
   "metadata": {},
   "source": [
    "Based off of the results from above, my hypothesis is **incorrect** and the model does in fact perform better on testing data when the hidden layer dimension is larger. Time and time again, the model with 64 dimensions in the hidden layer performed better than the model with 16 dimensions hidden layers. This is due to the fact that with a higher dimension in the hidden layer, the model has more computational ability to find patterns that will help it recognize unseen data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6df2b0-169d-48c4-8d04-9432cfeb8577",
   "metadata": {},
   "source": [
    "### 3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "\n",
    "If we replaced the ReLU activation function in the GCN layers with a sigmoid function, I would assume that the performance would decrease. This is due to the fact that ReLU is a simple activation problem where the output is either 0 or a positive number. With Sigmoid, the values can be in any range between 0 and 1 and can cause problems when training the layers of a neural network. Also, the ReLU is a lot faster computation wise than sigmoid. \n",
    "\n",
    "To test this out, I am going to train 2 models, one trained with ReLU activation and the other with sigmoid. I'm going to then evaluate the respective models on testing accuracy to see if the above statements are in fact true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40761477-65da-420e-8df9-0bb80caa3b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with ReLU Activation: Epoch 0, Loss: 2.9995131492614746, Accuracy: 0.2070000022649765\n",
      "Model with Sigmoid Activation: Epoch 0, Loss: 2.9995131492614746, Accuracy: 0.16899999976158142\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 10, Loss: 1.5442249774932861, Accuracy: 0.46299999952316284\n",
      "Model with Sigmoid Activation: Epoch 10, Loss: 1.5442249774932861, Accuracy: 0.781000018119812\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 20, Loss: 0.4719027280807495, Accuracy: 0.8019999861717224\n",
      "Model with Sigmoid Activation: Epoch 20, Loss: 0.4719027280807495, Accuracy: 0.7929999828338623\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 30, Loss: 0.10823110491037369, Accuracy: 0.7860000133514404\n",
      "Model with Sigmoid Activation: Epoch 30, Loss: 0.10823110491037369, Accuracy: 0.7820000052452087\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 40, Loss: 0.030787577852606773, Accuracy: 0.7889999747276306\n",
      "Model with Sigmoid Activation: Epoch 40, Loss: 0.030787577852606773, Accuracy: 0.777999997138977\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 50, Loss: 0.012459559366106987, Accuracy: 0.7900000214576721\n",
      "Model with Sigmoid Activation: Epoch 50, Loss: 0.012459559366106987, Accuracy: 0.7710000276565552\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 60, Loss: 0.006790930405259132, Accuracy: 0.7870000004768372\n",
      "Model with Sigmoid Activation: Epoch 60, Loss: 0.006790930405259132, Accuracy: 0.7689999938011169\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 70, Loss: 0.004522788338363171, Accuracy: 0.7900000214576721\n",
      "Model with Sigmoid Activation: Epoch 70, Loss: 0.004522788338363171, Accuracy: 0.7689999938011169\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 80, Loss: 0.003400693414732814, Accuracy: 0.7870000004768372\n",
      "Model with Sigmoid Activation: Epoch 80, Loss: 0.003400693414732814, Accuracy: 0.7670000195503235\n",
      "\n",
      "\n",
      "Model with ReLU Activation: Epoch 90, Loss: 0.0027122071478515863, Accuracy: 0.7889999747276306\n",
      "Model with Sigmoid Activation: Epoch 90, Loss: 0.0027122071478515863, Accuracy: 0.7689999938011169\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Creating a GCN with sigmoid activation\n",
    "\n",
    "class GCN_sigmoid(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN_sigmoid, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim = 1)\n",
    "\n",
    "# Initialize model with ReLU Activation\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize model with Sigmoid Activation\n",
    "model_sigmoid = GCN_sigmoid(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "optimizer_sigmoid = optim.Adam(model_sigmoid.parameters(), lr=0.01)\n",
    "criterion_sigmoid = nn.CrossEntropyLoss()\n",
    "        \n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model_sigmoid.train()\n",
    "    optimizer_sigmoid.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    out_sigmoid = model_sigmoid(data)\n",
    "    loss_sigmoid = criterion_sigmoid(out_sigmoid[data.train_mask], data.y[data.train_mask])\n",
    "    loss_sigmoid.backward()\n",
    "    optimizer_sigmoid.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Model with ReLU Activation: Epoch {epoch}, Loss: {loss.item()}, Accuracy: {test(model, data)}')\n",
    "        print(f'Model with Sigmoid Activation: Epoch {epoch}, Loss: {loss.item()}, Accuracy: {test(model_sigmoid, data)}')\n",
    "        print('\\n')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f25f4d-3c72-44c2-ab49-71c9f22aa2e2",
   "metadata": {},
   "source": [
    "As expected, the Sigmoid activation function does in fact reduce the performance of the model compared to the ReLU function, although not the extent I expected. I ran this experiment several times and sometimes the sigmoid activation did slightly better than the ReLU, but I am assuming that to be due to probability. After doing some more research, I learned that the reason the sigmoid function may be doing fine in our case is because the dataset is small and not too complex. In more complex situations, the ReLU would work far better. I also learned that sigmoid activation is better used on the output layer or if your network outpbinary, since all values in sigmoid are between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e225b1-9920-4c3a-b825-28474974067a",
   "metadata": {},
   "source": [
    "### 4) What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "\n",
    "Based off the previous homework, and some general knowledge, the performance should decrease significantly when trained on only 10% of the data.\n",
    "\n",
    "To test this, I will train a model using only 10% of the training nodes and test the results on the remaining 90% of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ef87742c-9d93-41eb-aaf6-10dc7752722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into 10 percent training and 90% testing\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "total_nodes = data.num_nodes\n",
    "ten_nodes = int(total_nodes * 0.10)\n",
    "rest_nodes = total_nodes - ten_nodes\n",
    "\n",
    "train_mask, test_mask = torch.zeros(total_nodes, dtype=torch.bool), torch.ones(total_nodes, dtype=torch.bool)\n",
    "\n",
    "train_idxs = torch.randperm(total_nodes)[:ten_nodes]\n",
    "\n",
    "for idx in train_idxs: \n",
    "    train_mask[idx] = True\n",
    "    test_mask[idx] = False\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9c5056fa-097e-43c9-991c-1b863a94c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.958348274230957\n",
      "Epoch 10, Loss: 0.7300834655761719\n",
      "Epoch 20, Loss: 0.18305355310440063\n",
      "Epoch 30, Loss: 0.055140767246484756\n",
      "Epoch 40, Loss: 0.02411525882780552\n",
      "Epoch 50, Loss: 0.01451635081321001\n",
      "Epoch 60, Loss: 0.01032477617263794\n",
      "Epoch 70, Loss: 0.007950633764266968\n",
      "Epoch 80, Loss: 0.006403629668056965\n",
      "Epoch 90, Loss: 0.005307072773575783\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fabe4-9596-4de4-8d61-b954fa701e96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "When the model is being trained only on 10% of the data, the model loss is around 0.005 while it is 0.002 otherwise. After looking more into this problem, I am slightly confused on why the model doesn't perform better. When I took a deeper look into the original model, I saw that is used only 140 training nodes (20 from each class) and 10% of all nodes is around 271 nodes. Hence, the network is actually being trained on more data than the original model and I would assume that the model would do better in turn. The only justification I can see for why the model with more data performs worse is due to the unequal proportions of training data that the model leading to biases being built in the model. With the smaller training set of 140 nodes, there was 20 nodes from each class and this was uniform, when using 10% of all nodes this wasn't necessarily the case and there could be more nodes in a certain class than another which could lead to error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb00b3-d8b5-45a0-8734-3be651ecc1d4",
   "metadata": {},
   "source": [
    "### 5) What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "\n",
    "I am not too familiar with the different optimizer, so I will first run an experiment testing how the different optimizers affect convergence speed and then see the differences they make. \n",
    "\n",
    "To test this, I will time how long it takes to run each model with the two different optimizers (RMSprop and Adam) and compare the results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e0c7961b-95d7-44ec-a2d3-92d90d50fee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.951730728149414\n",
      "Epoch 10, Loss: 0.561775267124176\n",
      "Epoch 20, Loss: 0.09437353163957596\n",
      "Epoch 30, Loss: 0.020149527117609978\n",
      "Epoch 40, Loss: 0.007373816799372435\n",
      "Epoch 50, Loss: 0.004144215025007725\n",
      "Epoch 60, Loss: 0.0029777614399790764\n",
      "Epoch 70, Loss: 0.0024123431649059057\n",
      "Epoch 80, Loss: 0.002070226240903139\n",
      "Epoch 90, Loss: 0.0018263454549014568\n",
      "Epoch 100, Loss: 0.0016348736826330423\n",
      "Epoch 110, Loss: 0.0014763379003852606\n",
      "Epoch 120, Loss: 0.001341241761110723\n",
      "Epoch 130, Loss: 0.001224307343363762\n",
      "Epoch 140, Loss: 0.0011221718741580844\n",
      "Epoch 150, Loss: 0.001032344182021916\n",
      "Epoch 160, Loss: 0.0009528992231935263\n",
      "Epoch 170, Loss: 0.0008823669631965458\n",
      "Epoch 180, Loss: 0.0008194185793399811\n",
      "Epoch 190, Loss: 0.0007629921892657876\n",
      "Epoch 200, Loss: 0.000712250592187047\n",
      "Epoch 210, Loss: 0.0006664280663244426\n",
      "Epoch 220, Loss: 0.0006249368889257312\n",
      "Epoch 230, Loss: 0.0005872652400285006\n",
      "Epoch 240, Loss: 0.0005529700429178774\n",
      "Epoch 250, Loss: 0.0005216271383687854\n",
      "Epoch 260, Loss: 0.0004929170245304704\n",
      "Epoch 270, Loss: 0.00046655844198539853\n",
      "Epoch 280, Loss: 0.00044227810576558113\n",
      "Epoch 290, Loss: 0.0004198785172775388\n",
      "Epoch 300, Loss: 0.00039917038520798087\n",
      "Epoch 310, Loss: 0.0003799857513513416\n",
      "Epoch 320, Loss: 0.00036216925946064293\n",
      "Epoch 330, Loss: 0.00034559014602564275\n",
      "Epoch 340, Loss: 0.00033015423105098307\n",
      "Epoch 350, Loss: 0.00031575869070366025\n",
      "Epoch 360, Loss: 0.0003023067838512361\n",
      "Epoch 370, Loss: 0.0002896981895901263\n",
      "Epoch 380, Loss: 0.000277894752798602\n",
      "Epoch 390, Loss: 0.0002667834924068302\n",
      "Training complete!\n",
      "Epoch 0, Loss: 0.0002563566667959094\n",
      "Epoch 10, Loss: 1.3676391972694546e-05\n",
      "Epoch 20, Loss: 1.3495940947905183e-05\n",
      "Epoch 30, Loss: 1.3305270840646699e-05\n",
      "Epoch 40, Loss: 1.3110342479194514e-05\n",
      "Epoch 50, Loss: 1.291456283070147e-05\n",
      "Epoch 60, Loss: 1.2711970157397445e-05\n",
      "Epoch 70, Loss: 1.2517040886450559e-05\n",
      "Epoch 80, Loss: 1.2313596016610973e-05\n",
      "Epoch 90, Loss: 1.210504342452623e-05\n",
      "Epoch 100, Loss: 1.1904150596819818e-05\n",
      "Epoch 110, Loss: 1.1690484825521708e-05\n",
      "Epoch 120, Loss: 1.1478520718810614e-05\n",
      "Epoch 130, Loss: 1.1271666153334081e-05\n",
      "Epoch 140, Loss: 1.1054590686399024e-05\n",
      "Epoch 150, Loss: 1.0841773473657668e-05\n",
      "Epoch 160, Loss: 1.0626398761814926e-05\n",
      "Epoch 170, Loss: 1.0405063221696764e-05\n",
      "Epoch 180, Loss: 1.019139017444104e-05\n",
      "Epoch 190, Loss: 9.97686311166035e-06\n",
      "Epoch 200, Loss: 9.769146345206536e-06\n",
      "Epoch 210, Loss: 9.54865481617162e-06\n",
      "Epoch 220, Loss: 9.33582577999914e-06\n",
      "Epoch 230, Loss: 9.118739399127662e-06\n",
      "Epoch 240, Loss: 8.90590854396578e-06\n",
      "Epoch 250, Loss: 8.693927156855352e-06\n",
      "Epoch 260, Loss: 8.481093573209364e-06\n",
      "Epoch 270, Loss: 8.273364983324427e-06\n",
      "Epoch 280, Loss: 8.06052685220493e-06\n",
      "Epoch 290, Loss: 7.85620250098873e-06\n",
      "Epoch 300, Loss: 7.654431101400405e-06\n",
      "Epoch 310, Loss: 7.455211289197905e-06\n",
      "Epoch 320, Loss: 7.256843218783615e-06\n",
      "Epoch 330, Loss: 7.062730674078921e-06\n",
      "Epoch 340, Loss: 6.860953362775035e-06\n",
      "Epoch 350, Loss: 6.673649295407813e-06\n",
      "Epoch 360, Loss: 6.488047347374959e-06\n",
      "Epoch 370, Loss: 6.304147518676473e-06\n",
      "Epoch 380, Loss: 6.1151376939960755e-06\n",
      "Epoch 390, Loss: 5.9423041420814116e-06\n",
      "Training complete!\n",
      "Adam Time: 11.24169659614563, RMSP Time: 10.90948748588562\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "adam_start = time.time()\n",
    "# Training loop\n",
    "for epoch in range(400):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "adam_end = time.time()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "model_rmsprop = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimzer_rmsprop = optim.RMSprop(model.parameters(), lr = 0.01)\n",
    "criterion_rmsprop = nn.CrossEntropyLoss()\n",
    "\n",
    "rmsp_start = time.time()\n",
    "# Training loop\n",
    "for epoch in range(400):\n",
    "    model_rmsprop.train()\n",
    "    optimzer_rmsprop.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimzer_rmsprop.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "rmsp_end = time.time()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(f\"Adam Time: {(adam_end - adam_start)}, RMSP Time: {(rmsp_end - rmsp_start)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45190e-f1a0-4e18-af42-91fd1c7adde3",
   "metadata": {},
   "source": [
    "Based off of the results above, the RMSprop optimzer is faster and converges faster than Adam! More specifically, in 400 epochs, RMSprop was able to get the loss down to 5.9423041420814116e-06 in 10.91 seconds, while Adam was able to get the loss down to 0.00026 in 11.242 seconds. Hence, RMSprop is the better optimzer for this problem since it is faster efficient, and converges faster!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
